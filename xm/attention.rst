Attention
=========


- The Unreasonable Effectiveness of Recurrent Neural Networks

    `<https://karpathy.github.io/2015/05/21/rnn-effectiveness/>`_

    An introduction to RNN with Python implementation.

- NLP From Scratch: Classifying names with a character-level RNN

  `<https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial>`_

- NLP From Scratch: Translation with a sequence to sequence network and attention

  `<https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`_

  It also shows

    - How to compute attention weight
    - How to visualize attention weight

- Sequence-to-sequence modeling with nn.transformer and torchtext

    `<https://pytorch.org/tutorials/beginner/transformer_tutorial.html>`_

    It says the transformer model is introduced in PyTorch 1.2

    It shows how PositionEncoding is implemented.

- Positional encoding in the transformer (a Chinese blog article)

    `<https://wmathor.com/index.php/archives/1453/>`_

- The Annotated Transformer

    `<http://nlp.seas.harvard.edu/2018/04/03/attention.html>`_
